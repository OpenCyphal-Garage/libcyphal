#!/usr/bin/env python3
#
# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.
# Copyright (C) 2018-2019  UAVCAN Development Team  <uavcan.org>
# This software is distributed under the terms of the MIT License.
#
"""
    Upload code from an lcov trace file to coveralls.io. cpp-coveralls appears
    to be broken and/or abandoned so we had to write this ourselves.
"""

import argparse
import copy
import hashlib
import json
import logging
import os
import pathlib
import re
import subprocess
import sys
import typing
from datetime import datetime, timezone

import requests


def gitlog(root_dir: pathlib.Path, fmt: str) -> str:
    return subprocess.run(['git', '--no-pager', 'log', '-1', '--pretty=format:{}'.format(fmt)],
                          check=True,
                          cwd=str(root_dir),
                          stdout=subprocess.PIPE).stdout.decode('utf-8')


def gitbranch(root_dir: pathlib.Path) -> str:
    return subprocess.run(['git', 'rev-parse', '--abbrev-ref', 'HEAD'],
                          check=True,
                          cwd=str(root_dir),
                          stdout=subprocess.PIPE).stdout.decode('utf-8')[:-1]


def gitremotes(root_dir: pathlib.Path) -> typing.List[str]:
    return subprocess.run(['git', 'remote', '-v'],
                          check=True,
                          cwd=str(root_dir),
                          stdout=subprocess.PIPE).stdout.decode('utf-8').splitlines()


def git_info(root_dir: pathlib.Path):
    """
    Git data that appears to be required to get source view from github to work properly.
    This is based on https://github.com/coveralls-clients/coveralls-python/blob/master/coveralls/git.py (MIT)
    """
    branch = (os.environ.get('APPVEYOR_REPO_BRANCH')
              or os.environ.get('BUILDKITE_BRANCH')
              or os.environ.get('CI_BRANCH')
              or os.environ.get('CIRCLE_BRANCH')
              or os.environ.get('GIT_BRANCH')
              or os.environ.get('TRAVIS_BRANCH')
              or os.environ.get('BRANCH_NAME')
              or gitbranch(root_dir))
    head = {
        'id': gitlog(root_dir, '%H'),
        'author_name': gitlog(root_dir, '%aN'),
        'author_email': gitlog(root_dir, '%ae'),
        'committer_name': gitlog(root_dir, '%cN'),
        'committer_email': gitlog(root_dir, '%ce'),
        'message': gitlog(root_dir, '%s'),
    }
    remotes = [{'name': line.split()[0], 'url': line.split()[1]}
               for line in gitremotes(root_dir)
               if '(fetch)' in line]

    return {
        'branch': branch,
        'head': head,
        'remotes': remotes,
    }


def upload_blocking(job: typing.Dict, args: argparse.Namespace) -> int:
    """
    A blocking HTTP(S) call to upload the provided job to coveralls.io.
    If args.dry_run is True no I/O is performed.
    """

    if args.dry_run:
        return 0

    coveralls_job_api = '{}/api/{}/jobs'.format(args.coveralls_endpoint, args.coveralls_api_version)

    if args.coveralls_gzip:
        import zlib
        compressed = zlib.compress(json.dumps(job).encode('utf-8'))

        response = requests.request('POST',
                                    coveralls_job_api,
                                    files={'json_file': ('json_file', compressed, 'gzip/json')},
                                    )
    else:
        response = requests.request('POST',
                                    coveralls_job_api,
                                    files={'json_file': json.dumps(job)},
                                    )

    try:
        result = {'status_code': response.status_code,
                  'body': response.json()
                  }
    except ValueError:
        result = {'status_code': response.status_code,
                  'body': response.text,
                  'reason': response.reason
                  }

    logging.getLogger().info(json.dumps(result, indent=4))

    return (0 if response.ok else response.status_code)


def create_section_from_sf(root_dir: pathlib.Path, name: str) -> typing.Dict[str, typing.Any]:
    """
    Initialize a new section object the first time we encounter it in a
    SF: record.
    """

    section = dict()  # type : typing.Dict[str, typing.Any]
    section['name'] = name
    coverage = []  # type: typing.List
    hash = hashlib.md5()
    full_path = root_dir / pathlib.Path(name)
    with open(str(full_path), 'rb') as source:
        for line in source:
            hash.update(line)
            coverage.append(None)
    section['source_digest'] = hash.hexdigest()
    section['coverage'] = coverage  # type: ignore
    section['branches'] = []  # type: ignore
    return section


def parse_info(root_dir: pathlib.Path, info_file_path: pathlib.Path) -> typing.List[typing.Dict]:  # noqa: C901
    """
    Parses an lcov trace file. See http://ltp.sourceforge.net/coverage/lcov/geninfo.1.php for documentation.
    """

    # TN:<test name>
    # SF:<absolute path to the source file>
    # FN:<line number of function start>,<function name>
    # FNDA:<execution count>,<function name>
    # FNF:<number of functions found>
    # FNH:<number of function hit>
    # BRDA:<line number>,<block number>,<branch number>,<taken>
    # BRF:<number of branches found>
    # BRH:<number of branches hit>
    # DA:<line number>,<execution count>[,<checksum>]
    # LH:<number of lines with a non-zero execution count>
    # LF:<number of instrumented lines>
    # end_of_record

    result = []  # type: typing.List[typing.Dict]

    with open(str(info_file_path), 'r') as info_file:
        sf_pattern = re.compile(r'SF:(\S*)')
        fn_pattern = re.compile(r'FN:(\d+),(\S+)')
        fnda_pattern = re.compile(r'FNDA:(\d+),(\S+)')
        da_pattern = re.compile(r'DA:(\d+),(\d+)')
        brda_pattern = re.compile(r'BRDA:(\d+),(\d+),(\d+),(\d+)')

        section = None  # type: typing.Optional[typing.Dict]

        # The function map is used to find line numbers for
        # function names. These are provided by FN: entries
        # whereas coverage.io only accepts line numbers for coverage
        # data.
        function_map = dict()  # type: typing.Dict[str, int]

        # we combine all test results for coveralls so we ignore TN and just accumulate
        # all execution for a given source file. See this resolution happening down in
        # the name_map lookup

        name_map = dict()  # type: typing.Dict[str, typing.Dict]
        for line in info_file:
            if line.startswith('end_of_record'):
                section = None
                continue

            sf_match = sf_pattern.match(line)
            if sf_match:
                name = str(pathlib.Path(sf_match.group(1)).relative_to(root_dir))
                try:
                    section = name_map[name]
                except KeyError:
                    section = create_section_from_sf(root_dir, name)
                    name_map[name] = section
                    result.append(section)
                continue

            if section is None:
                continue

            # +-[FN:]------------------------------------+
            fn_match = fn_pattern.match(line)
            if fn_match:
                lineno = int(fn_match.group(1))
                assert lineno > 0
                section['coverage'][lineno - 1] = 0
                function_map[fn_match.group(2)] = lineno
                continue

            # +-[FNDA:]----------------------------------+
            fnda_match = fnda_pattern.match(line)
            if fnda_match:
                lineno = function_map[fnda_match.group(2)]
                assert lineno > 0
                section['coverage'][lineno - 1] += int(fnda_match.group(1))
                continue

            # +-[DA:]------------------------------------+
            da_match = da_pattern.match(line)
            if da_match:
                lineno = int(da_match.group(1))
                assert lineno > 0
                execution_count = int(da_match.group(2))
                try:
                    section['coverage'][lineno - 1] += execution_count
                except TypeError:
                    section['coverage'][lineno - 1] = execution_count
                continue

            # +-[BFDA:]----------------------------------+
            brda_match = brda_pattern.match(line)
            if brda_match:
                section['branches'] += [int(brda_match.group(1)), int(brda_match.group(2)),
                                        int(brda_match.group(3)), int(brda_match.group(4))]

    return result


def _run(args: argparse.Namespace) -> int:
    """
    Parse and upload (if not --dry-run) to coveralls.io.
    See https://docs.coveralls.io/api-reference for API docs.
    """

    root_dir = pathlib.Path(args.root).resolve()

    info = parse_info(root_dir, args.trace_file)

    run_at_time = datetime.now(timezone.utc)
    run_at_time.replace(microsecond=0)

    job = {
        'repo_token': args.token,
        'service_name': args.service_name,
        'source_files': info,
        'parallel': False,
        'run_at': run_at_time.isoformat(),
        'git': git_info(root_dir)
    }

    if args.commit_sha is not None:
        job['commit_sha'] = args.commit_sha

    if args.build_number is not None:
        job['service_job_id'] = args.build_number

    if args.flag_name is not None:
        job['flag_name'] = args.flag_name

    if args.pull_request is not None:
        job['service_pull_request'] = args.pull_request

    logger = logging.getLogger()
    if logger.level <= logging.DEBUG:
        logger.debug(json.dumps(job, indent=4))
    elif logger.level <= logging.INFO:
        # elide the actual data.
        job_copy = copy.copy(job)
        job_copy['source_files'] = [item['name'] for item in job['source_files']]
        logger.info(json.dumps(job_copy, indent=4))

    return upload_blocking(job, args)


def _make_parser() -> argparse.ArgumentParser:
    """
        Defines the command-line interface.
    """

    epilog = '''**Example Usage**::

    export COVERALLS_TOKEN=32489-not-a-real-token-2390848q78d

    info_to_coveralls -b$BUILDKIT_BUILD_NUMBER build/tests/coverage.info

----
'''

    parser = argparse.ArgumentParser(
        description='Upload code from an lcov trace file to coveralls.io',
        epilog=epilog,
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)

    parser.add_argument('trace_file',
                        help='The lcov trace file.')

    parser.add_argument('--verbose', '-v', action='count',
                        help='verbosity level (-v, -vv)')

    parser.add_argument('--version', action='version', version='1.1')

    parser.add_argument('-b', '--build-number', default=os.environ.get('BUILDKITE_BUILD_NUMBER'), help='build number')

    parser.add_argument('-r', '--root', default=pathlib.Path.cwd(), help='The root directory for the repository.')

    parser.add_argument('--service-name', default=('cmake' if not os.environ.get('BUILDKITE') else 'buildkite'),
                        help='The coveralls.io service identifier. Can be anything but'
                             'travis-ci, travis-pro, and coveralls-ruby have special semantics.')

    parser.add_argument('-t', '--token', default=os.environ.get('COVERALLS_TOKEN'), help='The coveralls.io token for the project.')

    parser.add_argument('--dry-run', action='store_true', help='Parse but don\'t upload.')

    parser.add_argument('--commit-sha', required=False, help='The git commit.')

    parser.add_argument('--flag-name', required=False, help='An optional name for the job. Used to '
                        'create independent statuses within a single project.')

    parser.add_argument('-pr', '--pull-request', default=os.environ.get('BUILDKITE_PULL_REQUEST'), help='An optional pull request id '
                        'to mark this as a pull request build.')

    parser.add_argument('--coveralls-endpoint', default='https://coveralls.io', help='The host and prefix for the coveralls.io service.')

    parser.add_argument('--coveralls-api-version', default='v1', help='The coveralls REST API version.')

    parser.add_argument('--coveralls-gzip', action='store_true', help='If specified use gzip compression on uploads to coveralls endpoint.')

    return parser


def main() -> int:
    """
        Main entry point for this program.
    """

    args = _make_parser().parse_args()

    fmt = '%(message)s'
    level = {0: logging.WARNING, 1: logging.INFO,
             2: logging.DEBUG}.get(args.verbose or 0, logging.DEBUG)
    logging.basicConfig(stream=sys.stdout, level=level, format=fmt)

    return _run(args)


if __name__ == "__main__":
    sys.exit(main())
